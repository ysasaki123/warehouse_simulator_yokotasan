### 2/17

+ 倉庫の注文システムのシミュレータの依頼

バックグラウンド

+ イスラエルを本社におくプリンターメーカー（Kornit社）から業務提案の依頼あり（企業概要のPPTを添付します）。
+ Kornit社はTシャツへプリントをする業務用プリンターを製造販売しており、メインの顧客はAmazon。売上は堅調に伸びている。現状の課題は、修理用パーツの供給に遅れが生じていること。そのため、プリンターのDowntimeが発生し、Amazon他顧客に迷惑をかけている模様。
+ 当社としては、上記問題を解決するため、現状で1拠点（KY州のUPS倉庫から全米へ発送）のパーツ供給を、多拠点のパーツ管理・供給を提案したい（全米6箇所をイメージ）
+ 多拠点管理は弊社の得意とするところであり、現状のUPS社から仕事を取るには、複雑な多拠点でのパーツ管理の最適化を提案する必要あり。
+ 最初は人力で管理を行うが、徐々に以下のようなAIシステムで管理をしたい。
+ ちなみに、アイテム数は8000。そのうち、需要の高いパーツは1000。一日の発送件数は約100件


必要となるAI開発

+ 拠点6拠点のパーツ受発注管理
    + アイテム数8000件（×6拠点）
    + 強化学習で在庫レベルを最適化させたい
+ Action
    + 海外工場へのパーツ発注
    + 拠点間のパーツの融通
    + パーツの顧客への発送
+ Reward
    + パーツが在庫切れ（最適基準在庫を下回った）を生じた場合はPenalty
    + パーツが納期通りに顧客に到着しない場合はPenalty
    + スペース代（劣後事項なので当面無視）
    + 発送費用（劣後事項なので当面無視）
    + 拠点間のパーツ移動費用（劣後事項なので当面無視）


検討事項

+ 以前作って頂いたシステムは単一拠点を対象。今回、6拠点となると、その点で、解の安定がどうなるか？　試行錯誤必要
+ パーツごとの需要予測が別途必要だが、当面は、地域ごと、パーツごとの過去実績から単純予測することで対応
+ 工場からのパーツ納期、顧客までの配達時間などの要件は要確認
+ 基本的なロジックの部分を作って頂いて、UIなどはSardarさんに引き継ぐ形でお願いしたいと思っています。

### 2/17 ~ 4/9

+ 行ったこと
    + 強化学習する問題を作成・デバッグ

+ 解こうと考えた問題
    + State
        + アイテム数6件 x 3拠点
        + (受注量, 在庫量, 発注量, アイテムの発注リードタイム, アイテムの平均受注量)
    + Action
        + 海外工場へのパーツ発注 -> 発注 or 発注しない
        + 拠点間のパーツの融通 -> スコープから外した
        + パーツの顧客への発送 -> 在庫があれば自動で発送すると考えることで、Actionの選択肢から外した
    + Reward
        + 実際のお金が報酬となるように設計
            + 納品 -> プラスの報酬
            + 発注 -> マイナスの報酬
            + 在庫量 -> マイナスの報酬
        + 報酬は毎単位時間に発生

+ 苦戦した課題
    + 1単位時間をどのように設計するか？
        + 1単位時間を限定しなくても実装はできると考え、とりあえず、1単位時間を1時間や日数などと設定せず、1単位時間として実装した
    + 1単位時間にパーツの発注量をどのように決めさせるのか？
        + 1単位時間の中で、Actionを複数回実行できるようにし、注文量 = 注文数 x 20として計算することにより、Actionで量を決めさせる問題を回避
            + Q学習ではQ(s, a) -> v となる関数を学習させることが目標となるが、aが整数値になるとState x Actionの空間が広がり学習が困難になるのでは？と考えたため、Actionを`注文 or 注文完了`の2つに限定させることで学習が進むようになるのでは？と考えた

    + シミュレーターにバグがないか？の確認
        + 報酬の発生タイミングや、Q学習の方法など、バグがないのか？をテストすることが難しかった。理想な状況を想定し、問題をできる限り簡単にした上で、すべての状態価値を保存するメモ化を使った動的計画法を使って、細かく学習が進んでいるか？を確認しながらデバッグを行った

### 4/9 ~ 4/23

+ ある程度でバッグが完了した時点で、Q関数を機械学習させるアプローチで学習させてみようと考え、いくつかの機械学習のパターンを試してみた
    + 基本的な考え方としては、Deep Q Learningを模倣したアプローチを用いた
        1. シミュレータを使って経験(State, Action, Reward, NextState)を生成
        2. 経験を使ってQ関数の学習データを作成 State, Action -> Valueのデータを作成
        3. 教師あり学習の機械学習モデルを幾つか試して、Q関数を学習
        4. 学習したQ関数を使ってシミュレータを動かし、次の経験を生成
        5. 2~4を繰り返し、Q関数を更新していく

+ 試した機械学習モデル
    + Support Vector Regression
        + Support Vector Machineを使って回帰を行うモデル
        + Support Vector = 回帰モデルを作るために必要なデータのこと
            + 大量の経験から、回帰モデルを作るために重要な経験を抽出できるのでExperience Replayと相性が良いのでは？と考えた
        + 計算に時間がかかることが欠点
        + 実際にやってみると、他の手法と比べて時間がかかりすぎたため、何度も試行を繰り返す強化学習には向いていないと感じたため使わないことにした
    + Multi Layer Processing
        + 3 ~ 5層ぐらいの単純な多層ニューラルネットワーク
        + 隠れ層は全て全結合(Free Connect)
        + 学習時間に関しては早くはないが、試行錯誤をする上で問題には感じないレベルであった。GBDTと比べて結果を出すのに時間がかかったので、一旦こちらは使わないことにした

    + Gradient Boosting Decision Tree
        + 決定木を重ねて分類器を作る方法
        + 試した三つの機械学習の中で、一番学習時間が短かった。また、パッと見の学習結果もよく見えたので、こちらのモデルをとりあえず採用することにした

+ GBDTを使った学習の問題点
    + 逐次学習ではないので、経験が変わるとすぐに結果が変わってしまうことがわかった
        + メモリ量が覚えてられる経験の量の上限になる。アイテム数が増えると必要になる経験の量も増えるため、単純に経験を蓄積していくようなアプローチの場合は、アイテム数の増加に対するスケーラビリティがない
            + 覚える経験の量に上限を設けて、昔の経験から捨てていって新しい経験を増やしていくというアプローチで試してみたが、学習結果 <-> 経験を生成の繰り返しの中で、学習モデルが収束しないという現象が起きた
            + 覚える経験の量の上限に達した場合に、ランダムに経験を捨てるというアプローチも試してみたが、上記と同様の現象がおきた
            + 覚える経験をアイテムごとに分割して、アイテムごとに経験を覚えるというアプローチも試してみたが、アイテムごとでも不十分だった
            + 経験量を減らすために、注文単位を増やすというのも試してみた。(20刻みで注文するより50刻みで注文した方が経験の量が少なくなる。例えば、注文量を100まで増やす場合20刻みだと5つの経験が生成されるのに対し、50刻みだと2つの経験しか生成されない)が、改善が見られなかった

+ その他の問題点
    + 特徴量 (受注量, 在庫量, 発注量, アイテムの発注リードタイム, アイテムの平均受注量) だけだと、リードタイムが長い場合にActionが明確に決まらない
        + 例えば、後1日で工場からアイテムを受領できるケースでも、100日かかるケースでも同じ状態になりうる。前者の場合は発注しないようにすべきだが、後者は発注すべきである。

### 4/23 ~ 5/14

+ 学習モデルの問題ではなく、特徴量が足りないのでは？と考え、新たな特徴量を追加

+ 解こうと考えた問題
    + State
        + アイテム数6件 x 3拠点
        + (受注量, 在庫量, 発注量, アイテムの発注リードタイム, アイテムの平均受注量, アイテムの納期の統計量)
    + Action
        + 海外工場へのパーツ発注 -> 発注 or 発注しない
    + Reward
        + 実際のお金が報酬となるように設計
            + 納品 -> プラスの報酬
            + 発注 -> マイナスの報酬
            + 在庫量 -> マイナスの報酬
        + 報酬は毎単位時間に発生

+ 課題
    + 特徴量を一つ増やすと、思いの外GBDTの計算に時間がかかるようになってしまった
    + また、学習に必要な経験の量も増えてしまいGBDTだと学習がうまく行われなくなってしまった
        + 経験の量を減らすために、経験をランダムサンプルして学習を行うアプローチも試してみたが、うまくいかず

### 5/14 ~ 7/2

+ GBDTではなく、DeepLearningを使って逐次学習を行うことで経験のブレの影響を減らし、学習にかかる時間も短縮させる

+ 解こうと考えた問題
    + State
        + アイテム数6件 x 3拠点
        + (受注量, 在庫量, 発注量, アイテムの発注リードタイム, アイテムの平均受注量, アイテムの納期の統計量)
    + Action
        + 海外工場へのパーツ発注 -> 発注 or 発注しない
    + Reward
        + 実際のお金が報酬となるように設計
            + 納品 -> プラスの報酬
            + 発注 -> マイナスの報酬
            + 在庫量 -> マイナスの報酬
        + 報酬は毎単位時間に発生

+ 課題
    + 特徴量を増やした状態で学習することができるようになったが、学習がうまくいかない
        + 学習モデルの問題か、強化学習の問題のバグが原因、強化学習の問題設定の問題か調査

### 7/2 ~ 8/19

+ 強化学習の問題設定に問題があるのでは？と考え、報酬をシミュレーションの最後に与えるようにした
    + 行動の後すぐに報酬を与える方法だと、未来の報酬よりも直近の報酬からの影響をより多く受けてしまい、リードタイムが長い場合に対応できなくなっていたのではないか？

+ 解こうと考えた問題
    + State
        + アイテム数6件 x 3拠点
        + (受注量, 在庫量, 発注量, アイテムの発注リードタイム, アイテムの平均受注量, アイテムの納期の統計量)
    + Action
        + 海外工場へのパーツ発注 -> 発注 or 発注しない
    + Reward
        + 実際のお金が報酬となるように設計
            + 納品 -> プラスの報酬
            + 発注 -> マイナスの報酬
            + 在庫量 -> マイナスの報酬
        + 報酬はシミュレーション後に発生

+ 結果
    + うまくいかず。うまくいかない原因を考えるために、学習結果(パラメータと、予想結果)を可視化してみることにした
        + 可視化してみたところ、想定する学習ができていないことがわかった。おそらく、強化学習の問題設定か、モデルに問題がありそう

